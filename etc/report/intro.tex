\part{Algorithms}

\chapter{Introduction}
This report presents the development of a C++ library containing Markov chain sampling algorithms for two major goals: estimation of the density and clustering analysis of a given set of data points.
In a Bayesian nonparametric setting, we focused on the Dirichlet process, one of the most widely used priors due to its flexibility and computational ease, and its extensions.
Hereafter, we will assume that the underlying model for the given data points is a Dirichlet process mixture model, which is an enhancement of the simpler Dirichlet model.
We shall now briefly describe these models and their relevant properties.
(For a more detailed discussion of the nonparametric models, as well as references for all theoretical details included in this section, see \cite{book} chapter 1 and 2.)

\section{Dirichlet process model}
Let $M>0$, and let $G_0$ be a probability measure defined on the state space $S$.
A Dirichlet process with parameters $M$ and $G_0$, noted as $DP(M,G_0)$, is a random probability measure $G$ defined on $S$ which assigns probability $G(B)$ to every set $B$ such that for each finite partition ${B_1,\dots,B_k}$ of $S$, the joint distribution of the  vector $(G(B_1),\dots,G(B_k))$ is the Dirichlet distribution with parameters
\begin{align*}
(MG_0(B_1),\dots,MG_0(B_k)).
\end{align*}
The parameter $M$ is called the precision or total mass parameter, $G_0$ is the centering measure, and the product $MG_0$ is the base measure of the DP. \\
Having observed the independent and identically distributed sample $\{y_1,\dots,y_n\}$ \\*
$\subseteq \RR$, the basic DP model takes the following form:
\begin{equation}
	\begin{aligned}
	y_i | G &\iidsim G, \quad i=1,\dots,n \\
	G &\sim DP(MG_0)
	\end{aligned}
\end{equation}
A key property is that the DP is conjugate with respect to iid sampling, so that the posterior base distribution is a weighted average of the prior base distribution $G_0$ and the empirical distribution of the data, with the weights controlled by $M$:
\begin{align}
	G | y_1,\dots,y_n \sim DP\left(M G_0 + \sum_{i=1}^n \delta_{y_i}\right).
\end{align}
Moreover, the marginal distribution for the data will be the product of the sequence of increasing conditionals:
\begin{align*}
	p(y_1,\dots,y_n)= p(y_1)\prod\limits_{i=2}^{n} p(y_i|y_1,\dots,y_{i-1}),
\end{align*}
with $y_1 \sim G_0$ and the conditional for $i=2,3,\dots$ being the following:
\begin{align*}
	p(y_i|y_1,\dots,y_{i-1})= \frac{1}{M+i-1}\sum_{h=1}^{n-1} \delta_{y_h}(y_i) +\frac{M}{M+i-1} G_0(y_i).
\end{align*}
Another important property is the discrete nature of the random probability measure $G$.
Because of this, we can always write $G$ as a weighted sum of point masses.
A useful consequence of this property is its stick-breaking representation, i.e. $G$ can be written as:
\begin{align*}
	G(\cdot) = \sum_{k=1}^{+\infty} w_k \delta_{m_k} (\cdot),
\end{align*}
with $m_k \iidsim G_0$ for $k\in\mathbb{N}$ and the random weights constructed as $w_k =v_k\prod\limits_{l<k} (1-v_l)$ where $v_k \iidsim Be(1,M)$. \\
In many applications in which we are interested in a continuous density estimation, this discreteness can represent a limitation.
Oftentimes a Dirichlet process mixture (DPM) model is used, where the DP random measure is the mixing measure for the parameters of a parametric continuous kernel function.

\section{Dirichlet process mixture model}
Let $\Theta$ be a finite-dimensional parameter space and $G_0$ a probability measure on $\Theta$.
The Dirichlet process mixture (DPM) model convolves the densities $f(\cdot|\boldsymbol\theta)$ from a parametric family $\Fc = \{f(\cdot|\boldsymbol\theta), \boldsymbol\theta \in \Theta \}$ using the DP as mixture weights.
The obtained model has the following form:
\begin{equation}
	\begin{aligned}\label{dpm-1}
	y_i | G &\iidsim f_G(\cdot) = \int_\Theta f(\cdot|\boldsymbol\theta) \, G(\de\boldsymbol\theta), \quad i=1,\dots,n \\
	G &\sim DP(M G_0)
	\end{aligned}
\end{equation}
An equivalent hierarchical model is:
\begin{equation}
	\begin{aligned}\label{dpm-2}
	y_i | \boldsymbol\theta_i &\indsim f(\cdot|\boldsymbol\theta_i), \quad i=1,\dots,n \\
	\boldsymbol\theta_i | G &\iidsim G, \quad i=1,\dots,n \\ 
	G &\sim DP(M G_0)
	\end{aligned}
\end{equation}
where the \emph{latent variables} $\boldsymbol\theta_i$ are introduced, one per unit.
Since $G$ is discrete, we know that two independent draws $\boldsymbol\theta_i$ and $\boldsymbol\theta_j$ from $G$ can be equal with positive probability.
In this way the DPM model induces a probability model on clusters of $\boldsymbol\theta_i$.
An object of interest that derives from this model is the partitioning induced by the clustering. \\%, as well as the density estimation. \\
Considering $n$ data points, each $\boldsymbol\theta_i$ will have one of the $k$ unique values $\boldsymbol\phi_{j}$.
An estimation of the number of the unique values is $M\log(n) \ll n$.
Defining  $\boldsymbol c= (c_1,\dots,c_n)$ the \emph{allocation} parameters to the clusters such that $c_i = j$ if $\boldsymbol\theta_i = \boldsymbol\phi_j$, model (\ref{dpm-2}) can be thought of as the limit as $K \to +\infty$  of a finite mixture model with $K$ components (recall instead that $k$ is the number of unique values):
\begin{equation}
	\begin{aligned}\label{dpm-disc}
		y_{i}|\boldsymbol{\phi}_1,\dots,\boldsymbol{\phi}_k,c_{i} &\indsim f(\cdot|\boldsymbol\phi_{c_{i}}), \quad i=1,\dots,n \\
		c_{i}|\mathit{\mathbf{p}}&\iidsim \sum_{j=1}^K\mathit{p_j} \delta_j(\cdot), \quad i=1,\dots,n \\
		\boldsymbol\phi_{c} & \iidsim G_{0}, \quad c=1,\dots,k \\
		\mathbf{p} &\sim \operatorname{Dir}(M/K,\dots,M/K)
	\end{aligned}
\end{equation}
where $\mathbf{p}=(p_1,\dots,p_K)$ represents the mixing proportions for the clusters and each $\boldsymbol\theta_i$ is characterized by the latent cluster $c_i$ and the corresponding parameters $\boldsymbol\phi_{c_i}$.

\subsection{Normal Normal-InverseGamma model} \label{nnig}
A very common choice for the DPM model (\ref{dpm-1}) is the Normal Normal-InverseGamma (Normal-NIG) model, opting for a Normal kernel and the conjugate Normal-InverseGamma as base measure $G_0$. That is, letting $\boldsymbol\theta=(\mu,\sigma)$, we have:
\begin{equation}
	\begin{aligned}
		f(y|\boldsymbol\theta)&=N(y| \mu ,\sigma^2),  \\
		G_0(\boldsymbol\theta|\mu_0,\lambda_0, \alpha_0, 	\beta_0)
		&=N\left(\mu | \mu_0 ,\frac{\sigma^2} {\lambda_0}\right) \times \text{Inv-Gamma}(\sigma^2|\alpha_0, \beta_0 ).
	\end{aligned}
\end{equation}
Note that in this model we have a full prior for $\sigma^2$ and instead a prior for $\mu$ that is conditioned on the value of $\sigma^2$.
Thanks to conjugacy, the predictive distribution for a new observation $\widetilde{y}$ can be computed analytically, finding a Student's $t$ (see \cite{integral} section 3.5):
\begin{align*}
	p(\widetilde{y}|\mu_0,\lambda_0, \alpha_0, \beta_0) =
	\int_\Theta f(\widetilde{y}|\boldsymbol\theta) \, G_0(\de\boldsymbol\theta) =
	\text{t}_{\widetilde \nu}\left(\widetilde{y}|\widetilde{\mu},\widetilde{\sigma}\right)
\end{align*}
where the following parameters are set:
$$
	\widetilde{\nu}=2 \alpha_0, \quad
	\widetilde{\mu}=\mu_0, \quad
	\widetilde{\sigma}^2= \frac{\beta_0(\lambda_0+1)}{\alpha_0 \lambda_0}
$$
Moreover, the marginal distribution for a given observation has the same expression. \\
The posterior distribution is again a Normal-InverseGamma (see \cite{integral} section 3.3):
\begin{align*}
	p(\boldsymbol\theta|y_1,\dots,y_n,\mu_0,\lambda_0, \alpha_0, \beta_0)=N\left(\mu | \mu_n ,\frac{\sigma^2} {\lambda_0 + n}\right) \times \text{Inv-Gamma}(\sigma^2|\alpha_n, \beta_n )
\end{align*}
with:
$$
\mu_n=\frac{\lambda_0 \mu_0 \bar{y} + n}{\lambda_0 + n}, \quad \alpha_n= \alpha_0 + \frac{n}{2}, \quad \beta_n= \beta_0 + \frac{1}{2}\sum_{i=1}^{n} (y_i-\bar{y})^2 + \frac{\lambda_0 n(\bar{y}-\mu_0)^2}{2(\lambda_0 + n)}.
$$
