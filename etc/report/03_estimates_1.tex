\chapter{Estimates} \label{chap-esimates-1}
We recall that the aim of these chain-producing algorithms is to build estimates for both the actual clustering structure of the data and its probability distribution, i.e. their likelihood.
We shall now explain how we achieve such estimates.
Note that these will be identical regardless of the dimension of the data points.


\section{Cluster estimation}
Suppose we wish to estimate the real clustering of the data, assuming the DPM model holds true.
Since the proposed algorithms repeat the same steps for many iterations, they will run through just as many states, i.e. combinations of unique values and clusters, which represent a clustering structure.
Therefore, one might think that a first rough estimate for the real clustering could be the \emph{final clustering}, that is, the state values corresponding to the last iteration of the algorithm.
However, due to the oscillating behavior of the clusters (as we shall show later on by performing tests), the last clustering is far from being guaranteed to be the optimal one.
Instead, we chose to implement a \emph{least square} estimate.
More specifically, we examine the state values provided by each iteration, then we select the one that minimizes the squared posterior \emph{Binder's loss function}, which gives cost 0 to a pair of points which are correctly assessed to be in the same cluster, and cost 1 to a pair of points which are placed in the same cluster but are actually in different ones.
An equivalent approach (see \cite{beep-bayes} lecture on BNP clustering) is computing the so-called \emph{dissimilarity matrix} for each iteration, computing its sample mean over all iterations, and finding the iteration that is the closest to the mean with respect to the \emph{Frobenius norm}, which the sum of squares of the matrix entries. 
More specifically, for each iteration $k$, the dissimilarity matrix $D^{(k)}$ is a symmetric, binary $n$-by-$n$ matrix (where $n$ is the number of available observations) whose entries $D^{(k)}_{ij}$ are $1$ if datum $i$ and $j$ are placed in the same cluster at iteration $k$ and $0$ otherwise.
After each $D^{(k)}$ and the sample mean $\bar{D} = \frac{1}{K} \sum_k D^{(k)}$ are computed, where $K$ is the number of iterations (not counting the ones in the burn-in phase), the best clustering $\hat{k}$ is found by minimizing the Frobenius norm of the difference with $\bar{D}$:
$$
\hat{k} = \argmin_k \left\lVert D^{(k)} - \bar D \right\rVert_F^2 = \argmin_k \sum_{i,j} \left( D^{(k)}_{ij} - \bar{D}_{ij} \right)^2.
$$
Note that by virtue of the involved matrices being symmetric, the minimization of the latter summation is equivalent to the one computed only for those entries $(i,j)$ such that $i<j$.
One can prove that the described algorithms \emph{converge in mean}, but not in the single iterations.
That is, the last iterations of the algorithm have no higher probability than the first ones of being the ``best'' cluster estimate; in fact, the algorithm has quite the \emph{oscillating behavior} in its single-iteration estimates, starting with the number of clusters at each iteration, as we will see in the last part of the report.
Instead, it is the mean of all dissimilarity matrices that converges to the ``best'' clustering structure: the more iterations are run, the better the approximation provided by the mean becomes.
(The number of iterations required is actually surprisingly small: we will show that even a few hundreds are enough to get a good estimate.)
Since the mean itself is obviously not a valid dissimilarity, as it is not binary-valued, we choose the one valid iteration matrix that best approximates it.
This guarantees the correctness of this least square estimate, at it is the closest available approximation to the mean dissimilarity matrix.



\section{Density estimation}\label{dens-estim}
The other important application of iterative BNP algorithms is the estimation of the density according to which the data points are distributed.
This is done slightly differently in both the \verb|Neal2| and \verb|Neal8| algorithms, as the former can exploit the conjugacy of the hierarchical model.
Just like for the cluster estimate, the computation will need to access all iterations run by the algorithm.
In either algorithm, suppose that iteration $k$ has $J$ clusters, that is, $j=0:J-1$.
Given a point $y$, we compute the local estimate of the density, which is built only taking iteration $k$ into account:
\begin{equation}\label{localdens}
	\begin{aligned}
		\hat f^{(k)}(y) \ = \ \sum_j \frac{n^{(k)}_j}{M+n} f\left(y | \boldsymbol{\phi}^{(k)}_j\right) \ + \ \frac{M}{M+n} m(y)
	\end{aligned}
\end{equation}
where $n^{(k)}_j$ is the cardinality of cluster $j$.
That is, the local estimate is a weighted mean of the likelihood given the unique values $\boldsymbol{\phi}^{(k)}_j$ of cluster $j$ and the marginal distribution $m(y)$ computed in the point.
Again, note that the relative weights of the clusters are proportional to their size $n^{(k)}_j$, while the ``virtual'' cluster of the marginal counts as having size $M$, the total mass parameter ($n$ is the total number of observations, as per usual).
The marginal distribution is only known under the conjugacy assumption in the \verb|Neal2| algorithm.
In particular, for both an NNIG and an NNW model $m(y)$ is a Student's $t$ as explained in section \ref{nnig} onwards.
In the \verb|Neal8| algorithm, $m(y)$ is not available in closed form, and thus it is replaced in the above formula by the following approximation:
\begin{equation}\label{margneal8}
	\begin{aligned}
		\hat m(y) = \frac{1}{m} \sum_{h=0}^{m-1}  f\left(y | \boldsymbol{\phi}_h\right)
	\end{aligned}
\end{equation}
where we use $m$ unique values, that is, one for each of the $m$ auxiliary blocks of the algorithm, drawn from the base measure: $\boldsymbol{\phi}_{h} \overset{\text{iid}}{\sim} G_0, \ h=0:m-1$. \\
Finally, the total \emph{empirical density} is computed as the mean over all $K$ iterations:
$$
\hat f(y) = \frac{1}{K} \sum_k \hat f^{(k)}(y).
$$
Again, this estimate approaches the true posterior density of the data thanks to the convergence in mean of the chain.
