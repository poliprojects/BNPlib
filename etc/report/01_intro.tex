\part{Algorithms}

\chapter{Introduction}
This report presents the development of a C++ library containing Markov chain sampling algorithms for two major goals: estimation of the density and clustering analysis of a given set of data points.
In a Bayesian nonparametric setting, we focused on the Dirichlet process, one of the most widely used priors due to its flexibility and computational ease, and its extensions.
Hereafter, we will assume that the underlying model for the given data points is a Dirichlet process mixture model, which is an enhancement of the simpler Dirichlet model.
(For a more detailed discussion of the nonparametric models, as well as references for all theoretical details included in this section, see \cite{book} chapter 1 and 2.)


\section{A quick introduction to Bayesian statistics}
Bayesian statistics is a branch of mathematics with goals similar to regular statistics, which also holds the more accurate name of \emph{frequentist statistics}.
In both theories, data points $y$ are considered as realizations of random variables, often iid -- independent and identically distributed, with their distribution having one or more fixed and unknown parameters $\theta$, such as mean and variance in the Gaussian cases.
However, the frequentist approach is heavily focused on data and on pure information that can be extracted from it, for instance via estimates based on some form of sample mean.
By contrast, the Bayesian approach brings the data scientist's prior knowledge about the data into the picture; such knowledge is assumed to be approximately true, and the data is used to give it refinements and updates.
In a formal Bayesian setting, this prior knowledge takes on the form of a distribution $p(\theta)$ on the parameters of the data, called \emph{prior distribution}, or prior for short.
This is a crucial difference with respect to frequentist statistics, where parameters $\theta$ are unknown but assumed to be fixed, whilst in the Bayesian environment they are treated as \emph{random variables} for all intents and purposes.
This explains a major advantage of the Bayesian theory: it is naturally suited for non-pointwise estimates, mainly in the form of parameter distributions or their summary statistics, and therefore these are much easier to obtain than in the frequentist counterpart.
After taking the given data points into consideration, the parameter estimate provided by the prior is updated into the so-called \emph{posterior distribution} $p(\theta|y)$, or posterior for short, for the parameters.
The conditioning symbol indicates that the actual values of the realizations are used for a new better estimate of the parameters.
Similarly, since data are now distributed according to the random $\theta$, we use the notation $f(y|\theta)$ for the data distribution, which in this context is called \emph{likelihood}. \\
One can easily see that Bayesian statistics makes heavy use of conditional probabilities; so much so, in fact, that its very name is based off of the generalization of a well-known result for conditional probabilities, the \emph{Bayes theorem}.
In particular, this theorem states that, given $y_1, \dots, y_n$ iid random variables with joint likelihood $f(y|\theta) = f(y_1, \dots, y_n | \theta)$ and parameters with prior $p(\theta)$, the posterior for $\theta$ is given by
$$ p(\theta|y) = \frac{p(\theta) f(y|\theta)}{\int p(\theta) f(y|\theta) \, \dy} =
\frac{p(\theta) f(y|\theta)}{m(y)} \propto p(\theta) f(y|\theta).
$$
The denominator $m(y) = \int p(\theta) f(y|\theta) \, \dy$ is the \emph{marginal distribution} of data $y$, that is, its overall distribution without the knowledge of its parameters $\theta$.
It is often treated as an unimportant normalization constant, since it does not contain $\theta$, and therefore one often uses the last equality, which highlights the posterior's dependence on both the prior and the likelihood. \\
A coveted property for a Bayesian model is \emph{conjugacy}, that is to say that both the prior and the posterior distribution have the same form, e.g. they are both Gaussian distributions (most likely their parameters would both be different).
This property, or lack thereof, can make the difference between a posterior distribution being extremely easy to compute, and being flat out impossible to compute in closed form.
We shall see some examples of conjugate models later on in this section. \\
Finally, note that a Bayesian model may still employ frequentist tools to better incorporate data information into the prior; the most common example of this is setting the mean of the prior distribution as the sample mean of the given data.

\subsection{Advanced models} %TODO blargh
One is also allowed to use a more layered model, in which the parameters of the prior distribution, called \emph{hyperparameters}, also have prior distributions on them -- these are called \emph{hyperpriors}.
The result is as follows:
\begin{align*}
	y_1, \dots, y_n | \theta, \lambda &\iidsim f(y|\theta, \lambda) \\ %TODO can you remove lambda?
	\theta | \lambda &\sim p(\theta|\lambda) \\
	\lambda &\sim \Pi(\lambda)
\end{align*}
In fact, one can add as many layers as needed, adding priors to other priors' parameters, although one hyperprior like in the above model is generally considered enough to handle the complexity of most problems.
These are called \emph{hierarchical models}. \\
Another kind of advanced Bayesian structure is the so-called \emph{nonparametric model}, in which the entire likelihood is assumed to be random.
This means that there are infinitely many points which are randomly generated, that is, we have an infinite-dimensional parameter -- with a prior distribution for it, of course:
such a likelihood is an example of \emph{random probability measure}. \\
In the next pages, we will see some examples of elements composing these models, which can be divided into two classes: the \emph{mixture} component and the \emph{hierarchy} component.


\section{Dirichlet process model}
Let $M>0$, and let $G_0$ be a probability measure defined on the state space $S$.
A Dirichlet process with parameters $M$ and $G_0$, noted as $DP(M G_0)$, is a random probability measure $G$ defined on $S$ which assigns probability $G(B)$ to every set $B$ such that for each finite partition ${B_1,\dots,B_k}$ of $S$, the joint distribution of the  vector $(G(B_1),\dots,G(B_k))$ is the Dirichlet distribution with parameters $(MG_0(B_1),\dots,MG_0(B_k))$.
The value $M$ is called the \emph{total mass} or precision parameter, $G_0$ is the \emph{centering measure}, and the product $MG_0$ is the base measure of the DP. \\
Having observed the iid sample $\{y_1,\dots,y_n\} \subseteq \RR$, the basic DP model takes the following form:
\begin{equation}\label{dpm}
	\begin{aligned}
	y_i | G &\iidsim G, \quad i=1,\dots,n \\
	G &\sim DP(MG_0)
	\end{aligned}
\end{equation}
A key property is that the DP is conjugate with respect to iid sampling, so that the posterior base distribution is a weighted average of the prior base distribution $G_0$ and the empirical distribution of the data, with the weights controlled by $M$:
\begin{align}\label{dp-posterior}
	G | y_1,\dots,y_n \sim DP\left(M G_0 + \sum_{i=1}^n \delta_{y_i}\right).
\end{align}
Moreover, the marginal distribution for the data will be the product of the sequence of increasing conditionals:
\begin{align*}
	p(y_1,\dots,y_n)= p(y_1)\prod\limits_{i=2}^{n} p(y_i|y_1,\dots,y_{i-1}),
\end{align*}
with $y_1 \sim G_0$ and the conditional for $i=2,3,\dots$ being the following:
\begin{equation}
	\begin{aligned}
		p(y_i|y_1,\dots,y_{i-1}) =
		\frac{1}{M+i-1}\sum_{h=1}^{n-1} \delta_{y_h}(y_i) + \frac{M}{M+i-1} G_0(y_i).
	\end{aligned}
\end{equation}
The common pattern in both the above expression of the conditional and in the one for the posterior in (\ref{dp-posterior}) is that the centering measure $G_0$ is always given a weight proportional to $M$ (up to the normalizing constant $M+i-1$ in the former), whilst for the single points $y_i$, or the delta distribution centered in them, the weight is 1 for each datum, or $k$ if it has appeared $k$ times.
This is the heart of the so-called \emph{Polya's urn} representation model: the probability of a new value being equal to the ones before it is proportional to the number of times this value has appeared in the past, while the probability of the value being generated anew from $G_0$ is proportional to $M$.
Therefore, the more a value appears, the more likely it is for it to appear again in the future; instead, the total mass acts as the  ``cardinality'' of the action ``draw a new value''.
This is equivalent to having an urn which contains balls of different colors; each time a ball of a certain color is extracted, the ball is placed back into the urn alongside another new ball of the same color.
Furthermore, there's a chance proportional to $M$ that instead of extracting a ball from the urn, a ball of a random color, not necessarily one that is already present in the urn, is created and placed into the urn.
This urn will become relevant in the creation of the sampling algorithms implemented in this library. 
\\
Another important property is the discrete nature of the random probability measure $G$.
Because of this, we can always write $G$ as a weighted sum of point masses.
A useful consequence of this property is its stick-breaking representation, i.e. $G$ can be written as:
\begin{align*}
	G(\cdot) = \sum_{k=1}^{+\infty} w_k \delta_{m_k} (\cdot),
\end{align*}
with $m_k \iidsim G_0$ for $k\in\mathbb{N}$ and the random weights constructed as $w_k =v_k\prod\limits_{l<k} (1-v_l)$ where $v_k \iidsim Be(1,M)$. \\

One can replace the Dirichlet process in (\ref{dpm}) with any almost surely discrete random probability measure, for example a Pitman-Yor process (PY).
The PY process (see \cite{PY}, pp. 855-900) is a two-parameter extension of the Dirichlet process, parametrized by a discount parameter $0 \leq \alpha \leq 1$, a strength parameter $\theta > - \alpha$, and a base distribution $G_0$. 
Nevertheless, we will use the DP as our working example for the explanations in the next chapters. Developments are similar if another discrete process is used.

In many applications in which we are interested in a continuous density estimation, the discreteness can represent a limitation.
Oftentimes a Dirichlet process mixture (DPM) model is used, where the DP random measure is the mixing measure for the parameters of a parametric continuous kernel function.

\section{Dirichlet process mixture model}
Let $\Theta$ be a finite-dimensional parameter space and $G_0$ a probability measure on $\Theta$.
The Dirichlet process mixture (DPM) model convolves the densities $f(\cdot|\boldsymbol\theta)$ from a parametric family $\Fc = \{f(\cdot|\boldsymbol\theta), \boldsymbol\theta \in \Theta \}$ using the DP as mixture weights.
The obtained model has the following form:
\begin{equation}
	\begin{aligned}\label{dpm-1}
	y_i | G &\iidsim f_G(\cdot) = \int_\Theta f(\cdot|\boldsymbol\theta) \, G(\de\boldsymbol\theta), \quad i=1,\dots,n \\
	G &\sim DP(M G_0)
	\end{aligned}
\end{equation}
An equivalent hierarchical model is:
\begin{equation}
	\begin{aligned}\label{dpm-2}
	y_i | \boldsymbol\theta_i &\indsim f(\cdot|\boldsymbol\theta_i), \quad i=1,\dots,n \\
	\boldsymbol\theta_i | G &\iidsim G, \quad i=1,\dots,n \\ 
	G &\sim DP(M G_0)
	\end{aligned}
\end{equation}
where the \emph{latent variables} $\boldsymbol\theta_i$ are introduced, one per unit.
Since $G$ is discrete, we know that two independent draws $\boldsymbol\theta_i$ and $\boldsymbol\theta_j$ from $G$ can be equal with positive probability.
In this way the DPM model induces a probability model on clusters of $\boldsymbol\theta_i$.
An object of interest that derives from this model is the partitioning induced by the clustering. \\%, as well as the density estimation. \\
Considering $n$ data points, each $\boldsymbol\theta_i$ will have one of the $k$ unique values $\boldsymbol\phi_{j}$.
An estimation of the number of the unique values is $M\log(n) \ll n$.
Defining  $\boldsymbol c= (c_1,\dots,c_n)$ the \emph{allocation} parameters to the clusters such that $c_i = j$ if $\boldsymbol\theta_i = \boldsymbol\phi_j$, model (\ref{dpm-2}) can be thought of as the limit as $K \to +\infty$  of a finite mixture model with $K$ components (recall instead that $k$ is the number of unique values):
\begin{equation}
	\begin{aligned}\label{dpm-disc}
		y_{i}|\boldsymbol{\phi}_1,\dots,\boldsymbol{\phi}_k,c_{i} &\indsim f(\cdot|\boldsymbol\phi_{c_{i}}), \quad i=1,\dots,n \\
		c_{i}|\mathit{\mathbf{p}}&\iidsim \sum_{j=1}^K\mathit{p_j} \delta_j(\cdot), \quad i=1,\dots,n \\
		\boldsymbol\phi_{c} & \iidsim G_{0}, \quad c=1,\dots,k \\
		\mathbf{p} &\sim \operatorname{Dir}(M/K,\dots,M/K)
	\end{aligned}
\end{equation}
where $\mathbf{p}=(p_1,\dots,p_K)$ represents the mixing proportions for the clusters and each $\boldsymbol\theta_i$ is characterized by the latent cluster $c_i$ and the corresponding parameters $\boldsymbol\phi_{c_i}$.

\section{Normal Normal-InverseGamma hierarchy} \label{nnig}
A very common choice for the DPM model (\ref{dpm-1}) is the Normal Normal-InverseGamma (NNIG) hierarchy, opting for a Normal kernel and the conjugate Normal-InverseGamma as base measure $G_0$.
The InverseGamma is the distribution of a random variable $Y$ such that $Y^{-1}$ follows a Gamma distribution.
That is, letting $\boldsymbol\theta=(\mu,\sigma)$, we have:
\begin{equation}
	\begin{aligned}
		f(y|\boldsymbol\theta)&=N(y| \mu ,\sigma^2),  \\
		G_0(\boldsymbol\theta|\mu_0,\lambda_0, \alpha_0, 	\beta_0)
		&=N\left(\mu | \mu_0 ,\frac{\sigma^2} {\lambda_0}\right) \times \text{Inv-Gamma}(\sigma^2|\alpha_0, \beta_0 ).
	\end{aligned}
\end{equation}
Note that in this model we have a full prior for $\sigma^2$ and instead a prior for $\mu$ that is conditioned on the value of $\sigma^2$.
Thanks to conjugacy, the predictive distribution for a new observation $\widetilde{y}$ can be computed analytically, finding a Student's $t$ (see \cite{integral} section 3.5):
\begin{align*}
	p(\widetilde{y}|\mu_0,\lambda_0, \alpha_0, \beta_0) =
	\int_\Theta f(\widetilde{y}|\boldsymbol\theta) \, G_0(\de\boldsymbol\theta) =
	\text{t}_{\widetilde \nu}\left(\widetilde{y}|\widetilde{\mu},\widetilde{\sigma}\right)
\end{align*}
where the following parameters are set:
$$
	\widetilde{\nu}=2 \alpha_0, \quad
	\widetilde{\mu}=\mu_0, \quad
	\widetilde{\sigma}^2= \frac{\beta_0(\lambda_0+1)}{\alpha_0 \lambda_0}
$$
Moreover, the marginal distribution for a given observation has the same expression. \\
The posterior distribution is again a Normal-InverseGamma (see \cite{integral} section 3.3):
\begin{align*}
	p(\boldsymbol\theta|y_1,\dots,y_n,\mu_0,\lambda_0, \alpha_0, \beta_0)=N\left(\mu | \mu_n ,\frac{\sigma^2} {\lambda_0 + n}\right) \times \text{Inv-Gamma}(\sigma^2|\alpha_n, \beta_n )
\end{align*}
with:
$$
\mu_n=\frac{\lambda_0 \mu_0 \bar{y} + n}{\lambda_0 + n}, \quad \alpha_n= \alpha_0 + \frac{n}{2}, \quad \beta_n= \beta_0 + \frac{1}{2}\sum_{i=1}^{n} (y_i-\bar{y})^2 + \frac{\lambda_0 n(\bar{y}-\mu_0)^2}{2(\lambda_0 + n)}.
$$

\section{Normal Normal-Wishart hierarchy} \label{nnw}
The multivariate extension of the above hierarchy is the Normal Normal-Wishart (NNW) model:
\begin{equation}
	\begin{aligned}
		f(\textbf y|\boldsymbol\theta) &= N(\textbf y | \boldsymbol\mu ,T^{-1}), \\
		G_0(\boldsymbol\theta|\boldsymbol \mu_0, \lambda, T_0, \nu)
		&= N\left(\mu | \boldsymbol\mu_0, (\lambda T)^{-1}\right) \times \text{Wish}(T|T_0, \nu).
	\end{aligned}
\end{equation}
In this case $\boldsymbol\theta=(\boldsymbol\mu, T)$.
Note that the scale is parametrized in a different way compared to univariate case, with the second unique value being the precision matrix $T = \Sigma^{-1}$, the inverse of the covariance matrix.
As such, the Wishart distribution is used instead of its inverse counterpart. \\
This hierarchy has similar properties to the NNIG case, namely conjugacy and a marginal distribution with the form of a multivariate Student's $t$.
