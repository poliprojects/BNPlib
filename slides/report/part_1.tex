
 
 
\section{Introduction}
This report presents the development of a C++ Bayesian Non parametric library containing Marcov Chain sampling methods for density estimation and clustering. In a Bayesian Non-parametric setting we focused on the Dirichlet process (DP) and its extensions, one of the most widely used priors due to its flexibility and computational ease.

\section{Dirichlet Process}
\textbf{Formal definition} : Let $M>0$ and $G_0$ be a probability measure defined on S. A DP with parameters $(M,G_0)$ is a random probability measure $G$ defined on $S$ which assigns probability $G(B)$ to every (measurable) set B such that for each (measurable) finite partition ${B_1,...,B_k}$ of $S$, the joint distribution of the  vector $(G(B_1),...,G(B_k))$ is the Dirichlet distribution with parameters 

\begin{align}
(MG_0(B_1),...,MG_0(B_k)).
\end{align}

The parameter $M$ is called the precision or total mass parameter, $G_0$ is
the centering measure, and the product $MG_0$ is referred to as the base measure
of the DP.

The basic DP model has the form:

\begin{align*}
		y_i | G &\iidsim G, \quad i=1,\dots,n \\
		G &\sim DP(MG_0)
\end{align*}



A key property is that the DP is conjugate with respect to i.i.d sampling so that
the posterior base distribution is a weighted average of the prior base distribution $G_0$ and the empirical distribution of the data, with the weighting controlled by $M$ :

\begin{align}
	G | \mathbf{y} \sim DP(M G_0 + \sum_{i=1}^n \delta_{y_i}) .
\end{align}

And the marginal distribution will be the result of the product of the conditionals:

\begin{align}
	p(y_i|y_1,...,y_{i-1})= \frac{1}{M+i-1}\sum_{h=1}^{n-1} \delta_{y_h}(y_i) +\frac{M}{M+i-1} G_0(y_i).
\end{align}

An important property of the DP is the discrete nature of $G$. As a discrete
random probability measure we can always write $G$ as a weighted sum of point masses.
A useful  property based on the discrete nature of the process is his stick-breaking representation, i.e. $G$ can be written as:

\begin{align}
 G(\cdot) = \sum_{k=1}^{+\infty} w_k \delta_{m_k} (\cdot)
 \end{align}
 
 with $m_k \iidsim G_0$ and the random weights constructed as $w_k =v_k\prod\limits_{l<k} (1-v_l)$ where $v_k$ are independent Be(1,M)random variables.




In many applications in which we are interested in a continuous density estimation this discreteness can represents a limit.
 It's common choice to use a Dirichlet Process Mixture (DPM) model where  the DP random measure is the mixing measure for the parameters of a parametric continuous kernel function.


\section{Dirichlet Process Mixture Model}

Extending the DP by convolving G with a kernel F, the model will have the form:

\begin{align*}
			y_i | G &\sim F_G(y) = \int F(y,\theta) \, G(\de\theta), \quad i=1,\dots,n \\
			G &\sim DP(M G_0)
\end{align*}


An equivalent hierarchical model is:
\begin{align*}
			y_i | \theta_i &\indsim F(\cdot,\theta_i), \quad i=1,\dots,n \\
			\theta_i | G &\iidsim G, \quad i=1,\dots,n \\ 
			G &\sim DP(M G_0)
\end{align*}

where the \textit{latent variables} $\theta_i$ are introduced, one per unit. Since G is discrete, we know that two independent draws $\theta_i$ and $\theta_j$ from G can be equal with positive probability. In this way the DPM model induces a probability model on clusters and an object of interest starting from this model is the partitioning induced by the clustering as well as the density estimation. 


Considering n data units, each $\theta_i$ will have one of the $k$ unique values $\phi_{j}$. An estimation of the number of the unique values is $M\log(n) \ll n$. Calling  $c_i$ the \textit{allocation} parameters to the clusters such that $c_i = j$ if $\theta_i = \phi_j$ the model can be thought as the limit as K goes to infinity of
finite mixture model with $K$ components:

\begin{align*}
            	(Y_{i}|\mathbf{\phi},c_{i})&\sim F(\cdot,\phi_{c_{i}}) \\
            	(c_{i}|\mathit{\mathbf{p}})&\sim \sum_{k=1}^K\mathit{p_k} \delta_k(\cdot) \\
            	\phi_{c} & \sim G_{0} \\
            	\mathbf{p} &\sim \operatorname{Dir}(M/K,\dots,M/K)
\end{align*}
        

where $(p_1,...,p_K)$ represent the mixing proportions for the classes and each theta is defined by the latent class $c$ and the corresponding parameters $\phi_c$.


\subsection{Normal Normal-InverseGamma Model}
A very common choice is the Gaussian Mixture Model, opting for a Normal kernel and as base measure $G_0$ the conjugate Normal-InverseGamma:

\begin{align*}
            	k(y|\theta)&=N(y, \mu ,\sigma^2)  \\
            	G_0(\theta|\mu_0,\lambda_0, \alpha_0, \beta_0)&=N\left(\mu | \mu_0 ,\frac{\sigma^2} {\lambda_0}\right)\text{Inv-Gamma}(\sigma^2|\alpha_0, \beta_0 )
\end{align*}

Thanks to the conjugacy the predictive distribution for a new observation $\widetilde{y}$ can be found analytically:

\begin{align}
      p(\widetilde{y}|\mu_0,\lambda_0, \alpha_0, \beta_0)= \int k(\widetilde{y}|\theta)p(\theta, G_0)d\theta= \frac{1}{\widetilde{\sigma}} \text{t-Student}\left(\frac{\widetilde{y}-\widetilde{\mu}}{\widetilde{\sigma}} ,|\widetilde{v}\right)        	
\end{align}
where $\widetilde{v}=2 \alpha_0$, $\widetilde{\mu}=\mu_0$ and $\widetilde{\sigma}= \sqrt{\frac{\beta_0(\lambda_0+1)}{\alpha_0 \lambda_0}} $

The posterior distribution is again a Normal-InvGamma:

\begin{align}
          p(\theta|y,\mu_0,\lambda_0, \alpha_0, \beta_0)=N\left(\mu | \mu_n ,\frac{\sigma^2} {\lambda_0 + n}\right)\text{Inv-Gamma}(\sigma^2|\alpha_n, \beta_n )
\end{align}
with $\mu_n=\frac{\lambda_0 \mu_0 \bar{y} + n}{\lambda_0 + n}$ , $\alpha_n= \alpha_0 + \frac{n}{2} $ and $\beta_n= \beta_0 + \frac{1}{2}\sum_{i=1}^{n} (y_i-\bar{y})^2 + \frac{\lambda_0 n(\bar{y}-\mu_0)^2}{2(\lambda_0 + n)}$

\section{Methods}
 
Starting from the hierarchical model (n) a direct approach is simply drawing values for each $\theta_i$ from its conditional given the data and the other $\theta_j$, but as we discussed before there is an high probability for ties among them and this can result in a slow convergence. 

\subsection{Neal2}

To solve and make it more efficient Neal proposed, starting from the discrete model (n), a Gibbs sampling method, integrating out the mixing proportion $\textbf{p}$. Assuming the current state of Markov chain consists of $(c_1,...,c_n)$  and the component parameters $\phi_c$ for all $c$, the Gibbs sampler consists of drawing values for each $c_i$ given the conditional probabilities:


\begin{align*}
			\PP(c_{i}=c | \mathbf c_{-i}, y_{i},\boldsymbol\phi) \propto \frac{n_{-i,c} + M/{K}}{n-1+M} F(y_{i},\phi_{c}) 
\end{align*}

and consequently a new value for each $\phi_c$ given the data belonging to that class. The passage to the infinite case is done taking the limit of $K$ to infinity in the conditional distribution of $c_i$ that becomes:


\begin{align*}
			\PP(c_{i}=c | \mathbf c_{-i}, y_{i},\boldsymbol\phi) &\propto \frac{n_{-i,c} }{n-1+M} F(y_{i},\phi_{c}) \\
			\PP(c_{i}\neq c_{j} \text{ for all } j | \mathbf c_{-i}, y_{i},\boldsymbol\phi) &\propto \frac{M }{n-1+M} \int F(y_{i},\phi) \, G_0(\de\phi)
\end{align*}

and considering only the $\phi_c$ associated with some observation, keeping feasible the sampling. At this point the algorithm works iteratively sampling $c$ and $\phi$. For each observation $i$ $c_i$ is updated according to its conditional distribution. It can be set either to one of the other components currently associated with some observation or to a new mixture component.  If the new value of $c_i$ is different from all the other $c_j$ a value for $\phi_{ci}$ is drawn from the posterior distribution $H_i$, based on the prior $G_0$ and the single observation $y_i$. Then for all the classes the  sample for $\phi_c$ is done considering the posterior distribution based on the prior and all the observations belonging to the specific class.

The probability of setting $c_i$ to a new component involves the integral $\int F(y_{i},\phi) \, G_0(\de\phi)$, which is difficult in the non-conjugate case, as well as the sample from $H_i$.

\subsection{Neal8}
To handle non-conjugate priors Neal proposed a second Markov chain sampling procedure where the state is extended by the addition of auxiliary parameters.
This technique allows to update the $c_i$ avoiding the integration with respect to $G_0$.
 
In this case the prior for $c_i$ are:
\begin{align*}
            \hspace{-25pt}
            \vspace{-12pt}
                \text{If $c=c_j$ for some $j$: } \PP(c_i=c | \mathbf c_{-i}) &= \frac{n_{-i,c}}{n-1+M}  \\
                \PP(c_{i}\neq c_{j} \text{ for all } j) &=\frac{M }{n-1+M}                  
\end{align*}
            
            	
where the probability of selecting a new component is split among the $m$ auxiliary components.
Maintaining the same structure as the \textit{Algorithm $2$}, the \textit{Algorithm $8$} is composed of two steps, where the components of the Markov Chain state $(c,\phi)$ are repeatedly sampled. The first step scans all the observations and evaluates each $c_i$. If this is equal to another $c_j$ then all the auxiliary variables are drawn from $G_0$. If it is a singleton then it is linked to one of the auxiliary variable with the corresponding value of $\phi_c$ while the others are drawn as before from $G_0$. Then $c_i$ is updated according to the conditional probabilities:

\begin{displaymath}
		            	\hspace{-20pt}
		            	P(c_{i}=c | \mathbf c_{-i}, y_{i}, \phi_{1},\dots,\phi_{h}) \propto \begin{cases}  \frac{n_{-i,c}}{n-1+M}F(y_{i},\phi_{c}), & \mbox{for } 1 \leq c \leq k^{-} \\ \frac{M/m}{n-1+M}F(y_{i},\phi_{c}), & \mbox{for } k^{-}+1 < c \leq h
		            	\end{cases}
		            	\vspace{-5pt}
		            	\end{displaymath}

Once all the $\phi_c$ not associated anymore with any observation are discarded, the algorithm proceeds with the sampling for $\phi_c$ for all the classes.


\subsection{Blocked Gibbs}
Another Gibbs Sampling method applicable in the considered Bayesian hierarchical models is the one proposed by Hemant Ishwaran and Lancelot F. James, where the prior $P$ is assumed to be a finite dimensional stick-breaking measure allowing in this way to update blocks of parameters. A key point of the method is that it does not marginalize over the prior, instead, grouping more variables together, it samples from their joint distribution conditioned on all other variables. 

It needs to draw from the conditionals:
	\begin{align*}
			\boldsymbol\phi &\sim \Lc(\boldsymbol\phi | \mathbf{c}, \mathbf{y}) \\
			\mathbf{c} &\sim \Lc (\mathbf{c}| \boldsymbol\phi,\mathbf{p}, \mathbf{y}) \\
			\mathbf{p} &\sim \Lc (\mathbf{p}| \mathbf{c})
		\end{align*}

The draw for the unique values can be handled easily also in the non-conjugate case applying standard Markov Chain Monte Carlo methods.



\subsection{Hierarchical Extensions}
