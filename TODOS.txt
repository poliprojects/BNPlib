TODO LIST


SCALETTA
* Tests: test 4 (Scipy), test 5-8 (multivariate), run all tests, plots+index
* Doxygen doc
* Report
* Slides
* Varie piccole cose
* REDO CALLGRIND


TESTS by Mario
* Univariati (dati 1-3 già pronti):
1) y_i ~ 0.5 N(-3, 1) + 0.5 N(3, 1), i=1,... 200
   mu_0 = 0.0
   lambda = 0.1 (o 10, dipende dalla parametrizzazione)
   a, b = 2 (parametri della inv-gamma)
2) y_i ~ 0.9 N(-5, 1) + 0.1 N(5, 1), i=1,... 1000
   stessi parametri di prima
3) y_i ~ 0.3 N(-2, 0.8^2) + 0.3 N(0, 0.8^2) + 0.4 N(2, 1) i=1,... 200
4) y_i ~ 0.5 t(5, -5, 1 ) + 0.5 SkewNormal(2, -5, 1) i=1,... 400
   (per generare questi dati guardate il pacchetto python Scipy)

* Multivariati:
5) y_i ~ 0.5 N((-3, -3), I) + 0.5 N((3, 3), I) i=1, ... 400
6-8) dati come prima (media della prima componente mistura tutti -3, e della
     seconda tutti +3) all'aumentare della dimensione (d=2, 5, 10, 20)
     per dimensioni sopra il 10 non dovrebbe più funzionare granchè...
* plot delle densità stimate e densità vere
* riportate rand-index della partizione stimata (solo per dimensione 1 e 2,
  dovrebbe venire sempre alto tranne magari l'esempio con la t e SkewNormal, ma
  la densità dovrebbe venire bene uguale)


VARIE
* fix aux_unique_values (non generati nuovamente per ogni dato, ma una volta
  per iterazione)
* fix totalmass
* incorporate draw() and sample_given_data() common calculations
* dataless in console.py (vedi file) 
* cluster_estimate() with sparse matrices (?)
* plot Python: highest posterior density intervals (credible intervals), arviz
  (library), contour plot
* Spazi, a capo, etc
* Rename some files (api, output, dataless) and functions (draw, sample, prob)
** grep -R "[^ ] $" src/*
* Check all algorithm/hierarchy/mixture combinations, csv files, etc
* README (Virtual Machine etc)
* Doxygen docs
** Python files
* confronto delle performance con il pacchetto R 'BNPMix', per quanto riguarda
  i tempi computazionali. Attenti a che sampler scegliete in quel pacchetto: 
  di default usa uno slice sampler che non è molto efficiente, selezionate il
  sampler marginale ('MAR') e matchate gli iperparametri.


REPORT
* in inglese
* Introduzione bayes
** Polya urn: infinite variabili latenti
* parametri non nel costruttore solitamente sono lasciati in default
* a way to erase and add clusters more efficiently, e.g. with map
* storing cardinalities > recomputing them at each iteration
* set_state flag efficienza
* hyperparameters per la NNW: lambda piccolo (permette libertà), mu0 media
  empirica, nu = dim.dati + 3, sigma0 = tale che la sua media (nu * tau0) sia
  l'identità
* spiegare le flag di efficienza: -march=native -O3 -msse2 -funroll-loops
  -ftree-vectorize


LIBRERIA (?)
* setup.py, niente Makefile
